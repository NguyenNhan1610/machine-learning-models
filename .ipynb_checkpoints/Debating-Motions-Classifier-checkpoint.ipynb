{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debating Motions Classifier\n",
    "\n",
    "Classifies motions into categories such as 'Economics' and 'Politics'. \n",
    "\n",
    "**Multi-label classifier**: Each motion can be classified into multiple categories. E.g. a motion can be both of the category 'Economics' and 'Politics'.\n",
    "\n",
    "The list of categories is specified: it is used in the training set and defined in the code as the array ``labels``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/Users/jessica/anaconda/lib/python3.5\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 22)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "def read_csv_to_np_array(file_path, header=True):\n",
    "    \"\"\"\n",
    "    Reads data from a csv file and converts it to a numpy array.\n",
    "    Returns data, header\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    header_values = data.columns.values\n",
    "    data = np.array(data)\n",
    "    if header == True:\n",
    "        return header_values, data\n",
    "    else:\n",
    "        return data\n",
    "debating_file_path = \"/Users/jessica/GitHub/data-science/data/debatingmotions_sorted.csv\"\n",
    "header, debating_data = read_csv_to_np_array(debating_file_path)\n",
    "\n",
    "# Check data is of expected shape\n",
    "debating_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify sheet variables.\n",
    "# Should consider assigning ``total_labelled`` and ``col`` variables programatically from debating_data input.\n",
    "\n",
    "total_labelled = 188\n",
    "train_size = 120\n",
    "test_size = total_labelled - train_size\n",
    "motion_col = 16\n",
    "infoslide_col = 17\n",
    "category_col_start = 18\n",
    "category_col_end_plus_one = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this unlabelled? \n",
      " ['2016-01-23' 'IoNA' 'United Kingdom' 0 'York IV' 'Bethany Garry'\n",
      " 'Jennie Hope' 'Nissim Massarano' nan nan nan nan nan nan '2' '2'\n",
      " 'This house believes that the Labour Party should have worked to rehabilitate Tony Blairâ€™s image in its campaigning, prior to the 2015 General Election.'\n",
      " nan nan nan nan nan] \n",
      "\n",
      "[['2016-01-01' 'International' 'Greece' 3 'WUDC' 'Manos Moschopoulos'\n",
      "  'Arinah Najwa' 'Chris Bisset' 'John McKee' 'Josh Zoffer'\n",
      "  'Sarah Balakrishnan' 'Tasneem Elias' nan nan '5' '5'\n",
      "  'THBT the US should withdraw from East Asia and cede regional hegemony to China.'\n",
      "  nan 'International Relations' 'Security, War and Military' nan nan]\n",
      " ['2015-01-01' 'International' 'Malaysia' 3 'WUDC' 'Shafiq Bazari'\n",
      "  'Jonathan Leader Maynard' 'Danique Van Koppenhagen' 'Sebastian Templeton'\n",
      "  'Engin Arikan' 'Brett Frazer' 'Madeline Schultz' nan nan 'Masters_1'\n",
      "  'Masters_1'\n",
      "  'TH regrets the rise of art that celebrates gaining material wealth' nan\n",
      "  'Art and Culture' nan nan nan]\n",
      " ['2013-11-15' 'IoNA' 'United Kingdom' 2 'Cambridge IV' nan nan nan nan nan\n",
      "  nan nan nan 'https://www.facebook.com/events/599836316713988/' 'Open_Z'\n",
      "  'Open_Finals'\n",
      "  \"THBT it is in the West's interests for Assad to decisively win the Syrian civil war. \"\n",
      "  nan 'International Relations' 'Security, War and Military' nan nan]]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle labelled data \n",
    "labelled_data = debating_data[:total_labelled,:]\n",
    "unlabelled_data = debating_data[total_labelled:,:]\n",
    "\n",
    "# Check unlabelled data really is unlabelled\n",
    "print('Is this unlabelled?', '\\n', unlabelled_data[0], '\\n')\n",
    "\n",
    "# Shuffle data\n",
    "np.random.shuffle(labelled_data)\n",
    "np.random.shuffle(unlabelled_data)\n",
    "print(labelled_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Convert labels to binary vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = labelled_data[:,category_col_start:category_col_end_plus_one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "labels = [\n",
    "'Art and Culture',\n",
    "'Business',\n",
    "'Criminal Justice System',\n",
    "'Development',\n",
    "'Economics','Education',\n",
    "'Environment',\n",
    "'Family',\n",
    "'Feminism',\n",
    "'Freedoms',\n",
    "'Funny',\n",
    "'International Relations',\n",
    "'LGBT+',\n",
    "'Media',\n",
    "'Medical Ethics',\n",
    "'Minority Communities',\n",
    "'Morality',\n",
    "'Politics',\n",
    "'Religion',\n",
    "'Science and Technology',\n",
    "'Security, War and Military',\n",
    "'Social Policy',\n",
    "'Social Movements',\n",
    "'Sports',\n",
    "'Terrorism',\n",
    "'The Human Experience'\n",
    "]\n",
    "\n",
    "len_labels = len(labels)\n",
    "print(len_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_vector = np.zeros(shape=(total_labelled,len_labels))\n",
    "for i in range(total_labelled):\n",
    "    for j in range(category_col_end_plus_one - category_col_start):\n",
    "        for k in range(len_labels):\n",
    "            if Z[i,j] == labels[k]:\n",
    "                Y_vector[i,k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform Ys into binary\n",
    "Y_dict = dict.fromkeys(labels)\n",
    "for i in range(len_labels):\n",
    "    Y_dict[labels[i]] = Y_vector[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dict['Art and Culture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose category to train for\n",
    "chosen_element = 'International Relations'\n",
    "Y_for_element = Y_dict[chosen_element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure training set\n",
    "\n",
    "X = labelled_data[:train_size,motion_col]\n",
    "Y = Y_for_element[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test set\n",
    "\n",
    "X_test = labelled_data[train_size:total_labelled,motion_col]\n",
    "Y_test = Y_for_element[train_size:total_labelled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THR the criminalization of the reckless transmission of sexual infections (e.g. HIV, herpes and gonorrhoea) in England and Wales'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that data is in form we expect\n",
    "X[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 836)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'house')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinements to occurrence count\n",
    "1. **tf**: To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "2. **tf-idf** (Term Frequency times Inverse Document Frequnecy) Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 836)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 836)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'thw criminalise drugs' => 0.0\n",
      "'THW ban endangered animals' => 0.0\n",
      "'ban drug taking in sports' => 0.0\n",
      "'THBT Saudi Arabia should nationalise its oil industry' => 1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict the outcome on a new document\n",
    "docs_new = ['thw criminalise drugs', 'THW ban endangered animals',\n",
    "           'ban drug taking in sports', 'THBT Saudi Arabia should nationalise its oil industry']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hard-coded array\n",
    "\n",
    "category_clfs = [text_clf, text_clf, text_clf, text_clf, \n",
    "                 text_clf, text_clf, text_clf, text_clf,\n",
    "                 text_clf, text_clf, text_clf, text_clf,\n",
    "                 text_clf, text_clf, text_clf, text_clf,\n",
    "                 text_clf, text_clf, text_clf, text_clf,\n",
    "                 text_clf, text_clf, text_clf, text_clf,\n",
    "                 text_clf, text_clf]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len_labels):\n",
    "    category_clfs[i] = text_clf.fit(X, Y_dict[labels[i]][:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of category classifiers\n",
    "category_clfs_dict = dict.fromkeys(labels)\n",
    "for i in range(len_labels):\n",
    "    category_clfs_dict[labels[i]] = text_clf.fit(X, Y_dict[labels[i]][:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art and Culture 0.941176470588\n",
      "Business 0.897058823529\n",
      "Criminal Justice System 0.882352941176\n",
      "Development 0.955882352941\n",
      "Economics 0.779411764706\n",
      "Education 0.955882352941\n",
      "Environment 0.926470588235\n",
      "Family 1.0\n",
      "Feminism 0.897058823529\n",
      "Freedoms 0.955882352941\n",
      "Funny 0.985294117647\n",
      "International Relations 0.764705882353\n",
      "LGBT+ 0.985294117647\n",
      "Media 0.970588235294\n",
      "Medical Ethics 0.970588235294\n",
      "Minority Communities 0.970588235294\n",
      "Morality 0.897058823529\n",
      "Politics 0.808823529412\n",
      "Religion 0.955882352941\n",
      "Science and Technology 0.985294117647\n",
      "Security, War and Military 0.764705882353\n",
      "Social Policy 0.926470588235\n",
      "Social Movements 0.970588235294\n",
      "Sports 1.0\n",
      "Terrorism 0.985294117647\n",
      "The Human Experience 0.985294117647\n",
      "Overall Mean Accuracy across Categories:  0.927601809955\n"
     ]
    }
   ],
   "source": [
    "# v1 Evaluation of the performance on the test set\n",
    "# v1 Evaluation of the performance on the test set\n",
    "import numpy as np\n",
    "\n",
    "def mean_accuracy_across_categories():\n",
    "    \"\"\"\n",
    "    Prints mean accuracy of predictions.\n",
    "    \"\"\"\n",
    "    overall_mean_accuracy_across_categories = []\n",
    "    \n",
    "    # For each category\n",
    "    for category_to_test in labels:\n",
    "\n",
    "        # Ask model to predict whether or not motions in ``X_test`` \n",
    "        # belong to category ``category_to_test``.\n",
    "        predicted = category_clfs_dict[category_to_test].predict(X_test)\n",
    "        \n",
    "        # Calculate the mean accuracy of these predictions \n",
    "        # for ``category_to_test``\n",
    "        category_mean_accuracy = np.mean(\n",
    "        \tpredicted == Y_dict[category_to_test][train_size:total_labelled])\n",
    "        print(category_to_test, category_mean_accuracy)\n",
    "        \n",
    "        # Append the mean accuracy to the array \n",
    "        # containing mean accuracy for all categories\n",
    "        overall_mean_accuracy_across_categories.append(category_mean_accuracy)\n",
    "    \n",
    "    # Calculate and print the mean accuracy across all categories \n",
    "    # and motions in the test set. \n",
    "    print('Overall Mean Accuracy across Categories: ', \n",
    "    \t\tnp.mean(overall_mean_accuracy_across_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Per Motion:  0.927601809955\n"
     ]
    }
   ],
   "source": [
    "# v1.5 Evaluation of the performance on the test set\n",
    "\n",
    "def accuracy_by_category(return_predictions=False):\n",
    "    \"\"\"\n",
    "    Returns an array of arrays of booleans that indicates whether each \n",
    "    prediction matched the true value.\n",
    "    Each row is a category and each value within the row is a motion.\n",
    "    \n",
    "    Weighs false positives the same as false negatives.\n",
    "    \n",
    "    >>> accuracy_by_category(e.g. if the prediction was 1 (in category)\n",
    "    but the true value was 0 (not in category), the value is False.\n",
    "    \n",
    "    if ``return_predictions=True``, returns array of arrays of predictions \n",
    "    as well.\n",
    "    \"\"\"\n",
    "    accuracy_by_category = []\n",
    "    predictions = []\n",
    "\n",
    "    # For each category\n",
    "    for category_to_test in labels:\n",
    "\n",
    "        # Generate predictions for all motions in test set\n",
    "        # as to whether or not a motion belongs to a category\n",
    "        predicted = category_clfs_dict[category_to_test].predict(X_test)\n",
    "\n",
    "\t\t# Add this to our array of predictions       \n",
    "        predictions.append(predicted)\n",
    "        \n",
    "        # Accuracy: Is the prediction the same as the true label?\n",
    "        accuracy_by_category.append(\n",
    "        \tpredicted == Y_dict[category_to_test][train_size:total_labelled])\n",
    "    \n",
    "    # If we want to, the function can return the array of predictions\n",
    "    # as well as the accuracy by category. \n",
    "    # (Specify ``return_predictions=True`` in func argument)\n",
    "    if return_predictions == True:\n",
    "        return accuracy_by_category, predictions\n",
    "    else:\n",
    "        return accuracy_by_category\n",
    "\n",
    "def mean_accuracy_per_motion():\n",
    "    \"\"\"\n",
    "    Prints the mean accuracy per motion.\n",
    "    \n",
    "    Assumes we weigh false positives the same as false negatives.\n",
    "    \"\"\"\n",
    "    accuracy_per_motion = []\n",
    "    acc_by_category = accuracy_by_category()\n",
    "\n",
    "    # For each motion\n",
    "    for i in range(test_size):\n",
    "        score = 0\n",
    "\n",
    "        # For the per-category prediction for each motion\n",
    "        for j in range(len(labels)):\n",
    "\n",
    "        \t# Add 1 to the score if the prediction was accurate,\n",
    "        \t# add 0 to the score if the prediction was inaccurate\n",
    "            score += acc_by_category[j][i]\n",
    "\n",
    "        # Normalise the score such that it's between 0 and 1 inclusive    \n",
    "        accuracy_for_one_motion = score/len(labels)\n",
    "\n",
    "        # Put all the accuracy scores into one array\n",
    "        accuracy_per_motion.append(accuracy_for_one_motion)\n",
    "\n",
    "    print('Mean Accuracy Per Motion: ', np.mean(accuracy_per_motion))\n",
    "\n",
    "mean_accuracy_per_motion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Suppose we weigh each false negative with weight ``false_neg`` and \n",
    "# each false positive with weight ``false_pos``. \n",
    "false_neg = 5\n",
    "false_pos = 1\n",
    "\n",
    "# v2 Evaluation of the performance on the test set\n",
    "\n",
    "def mean_score_per_motion_v2():\n",
    "    \"\"\"\n",
    "    Prints the mean score per motion.\n",
    "    \"\"\"\n",
    "    # Booleans, actual predictions. \n",
    "    # Rows: categories. Values within rows: motions.\n",
    "    acc_by_category, predicted = accuracy_by_category(return_predictions=True)\n",
    "    \n",
    "    # Initialise counts\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    accuracy_per_motion = []\n",
    "    \n",
    "    # For each motion\n",
    "    for i in range(test_size):\n",
    "        score = 0\n",
    "\n",
    "        # For the per-category prediction for each motion\n",
    "        for j in range(len(labels)):\n",
    "            \n",
    "            # Was the prediction accurate?\n",
    "            pred_accuracy = acc_by_category[j][i]\n",
    "\n",
    "            # We need the prediction to see if it is a \n",
    "            # true positive or negative\n",
    "            pred = predicted[j][i]\n",
    "            \n",
    "            # If the prediction is accurate\n",
    "            if pred_accuracy == 1:\n",
    "                score += 1\n",
    "                \n",
    "                # True positive\n",
    "                if pred == 1:\n",
    "                    tp += 1\n",
    "                \n",
    "                # True negative\n",
    "                elif pred == 0:\n",
    "                    tn += 1\n",
    "            \n",
    "            # Else if the prediction is not accurate\n",
    "            elif pred_accuracy == 0:\n",
    "                \n",
    "                # False positive\n",
    "                if pred == 1:\n",
    "                    score -= false_pos\n",
    "                    fp += 1\n",
    "                \n",
    "                # False negative\n",
    "                elif pred == 0:\n",
    "                    score -= false_neg\n",
    "                    fn += 1\n",
    "                    \n",
    "        # Normalise the score\n",
    "        score_for_one_motion = score/len(labels)\n",
    "\n",
    "        # Put all the scores into one array\n",
    "        score_per_motion.append(score_for_one_motion)\n",
    "\n",
    "    print('Mean Score Per Motion: ', np.mean(score_per_motion))\n",
    "    print('True Positives: ', tp, '\\n', 'True Negatives: ', tn, '\\n', \n",
    "    \t\t'False Positives: ', fp, '\\n', 'False Negatives: ', fn)\n",
    "\n",
    "mean_score_per_motion_v2()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course the two are the same. We're taking the means of all the values in the array of arrays ``accuracy_by_category``. But we have laid the foundations to vary the weightings of false positives and false negatives and come up with a different measure of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict categories for unlabelled motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art and Culture :  [ 0.]\n",
      "Business :  [ 0.]\n",
      "Criminal Justice System :  [ 0.]\n",
      "Development :  [ 0.]\n",
      "Economics :  [ 0.]\n",
      "Education :  [ 0.]\n",
      "Environment :  [ 0.]\n",
      "Family :  [ 0.]\n",
      "Feminism :  [ 0.]\n",
      "Freedoms :  [ 0.]\n",
      "Funny :  [ 0.]\n",
      "International Relations :  [ 0.]\n",
      "LGBT+ :  [ 0.]\n",
      "Media :  [ 0.]\n",
      "Medical Ethics :  [ 0.]\n",
      "Minority Communities :  [ 0.]\n",
      "Morality :  [ 0.]\n",
      "Politics :  [ 0.]\n",
      "Religion :  [ 0.]\n",
      "Science and Technology :  [ 0.]\n",
      "Security, War and Military :  [ 0.]\n",
      "Social Policy :  [ 0.]\n",
      "Social Movements :  [ 0.]\n",
      "Sports :  [ 0.]\n",
      "Terrorism :  [ 0.]\n",
      "The Human Experience :  [ 0.]\n"
     ]
    }
   ],
   "source": [
    "# Predict the outcome on a new motion\n",
    "motions_new = ['schools teachers students politicians elections government China']\n",
    "X_new_counts = count_vect.transform(motions_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "for category_to_test in labels:\n",
    "    predicted = category_clfs_dict[category_to_test].predict(motions_new)\n",
    "    print(category_to_test, \": \", predicted)\n",
    "    if predicted == 1:\n",
    "        print(\"Yes: \", category_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
